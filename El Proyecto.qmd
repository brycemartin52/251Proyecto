---
title: "Project"
author: "Derek & Bryce"
format: html
editor: visual
---

## Data Prep

```{r}
library(brms)
```

```{r}
# Original vectors
N <- c(9663, 117, 76, 116, 15, 71, 115, 1, 144, 473, 327, 94, 90, 6, 178, 61, 480, 432, 513, 243, 4492, 140, 112, 362, 632, 57, 371, 59, 2339, 40)
logN <- log(N)

Rating1 = c(4.6, 3.4, 3.8, 4.2, 3.5, 4.2, 3.2, 1.01, 3.9, 4.4, 4.8, 3.4, 4.6, 4.99, 4.4, 4.6, 4.3, 4.5, 2.6, 3.4, 4.8, 4.9, 3, 4.5, 4.4, 4.6, 3.8, 3.1, 4.5, 4.8)

pop1 <- data.frame(
  Rating = Rating1,
  logN = logN,
  Population = "Pop1"
)


N <- c(55, 78, 103, 59, 1596, 54, 12, 26, 1, 310, 8, 1448, 1, 40, 1, 13, 51, 28, 25, 2388, 135, 82, 15, 48, 66, 21, 29, 1112, 951, 91)
logN <- log(N)
Rating2 = c(4.5, 4.9, 4.3, 4.8, 4.2, 4.3, 4.99, 4.99, 4.99, 3.9, 4.99, 4.1, 4.99, 4.6, 1.01, 4.8, 4.5, 4.9, 4.8, 4, 4.5, 3.7, 4, 4.3, 3.8, 4.99, 4.99, 4.7, 3.9, 4.3)

pop2 <- data.frame(
  Rating = Rating2,
  logN = logN,
  Population = "Pop2"
)

data <- rbind(pop1, pop2)
```

We expect our model to

```{r}
# Fit Bayesian linear regression model
fit <- brm(
  formula = Rating ~ logN + (1 + logN | Population),  # Random intercept and slope for Population
  data = data,
  family = gaussian(),  # Normal likelihood
  prior = c(
    prior(normal(4, 2), class = "Intercept"),
    prior(normal(0, 10), class = "b"),
    prior(cauchy(0, 20), class = "sd")  # Prior on group-level standard deviations
  ),
  iter = 4000,  # Number of iterations
  chains = 4,   # Number of MCMC chains
  seed = 123    # For reproducibility
)
```

```{r}
summary(fit)
```

Given the output of the confidence intervals, we can not confidently say that the log(n) has any significant correlative effect on the ratings.

We are therefore skip trying to estimate that as a parameter and estimate only two things for each population: the true ratings for each and the standard deviation of those ratings.

Because there isn't a known posterior distribution for our rating data, we will use a Monte Carlo approximation to analyze our data. Our prior will be an uninformative uniform distribution, and our likelihood we assume a good approximation may be the beta distribution. As we acknowledge that the beta distribution doesn't allow for the endpoints, we will slightly change the data on the endpoints (5 star to 4.99, 1 star to 1.01). Since we aren't predicting how any individual would rate the store but the stores overall rating, we can confidently say that a given true store's rating is not equal to exactly 1 or 5 stars. This allows us to use the beta distribution for our likelihood.

Our model for a single store would be:

$$
x_i = \text{Average rating for the ith store at a specific mall}
\\
f(Data | \mu, \sigma^2) \sim Beta(3,1.5)
$$

```{r}
library(invgamma)

# PRIOR PARAMETERS
# Prior parameters for mu: 
alpha <- 3
beta <- 1.5

# Prior parameters for sigma2: 
gamma <- 2.01
phi <- 0.0001
#prior expected value of variance: 
phi/(gamma-1)

# Plot the prior distributions to make sure they seem reasonable
par(mfrow=c(1,2))

curve(1+4*dbeta(x, alpha, beta), xlim=c(1, 5), ylab="prior density", main=expression(pi(mu)), xlab=expression(mu))

curve(dinvgamma(x, gamma, phi), xlim=c(0, .0005), ylab="prior density", main=expression(pi(sigma^2)), xlab=expression(sigma^2))
```

```{r}
# COLLECT DATA
pop1 <- Rating1
pop2 <- Rating2

n <- length(pop1)

# POSTERIOR DISTRIBUTIONS: Must use Gibbs Sampling Algorithm to approximate

#Starting values (This example has extreme starting values just for illustrative purposes)
mu <- 4 #-200  #more reasonable starting value: 1.5
sigma2 <- 1 #10000 #more reasonable starting value: 0.001

library(MASS)

# Set the shape and rate parameters for the gamma distribution
alpha = 3
beta = 1.5

# Generate alpha and beta parameters using the gamma distribution


# Print the generated alpha and beta parameters

# initializations for the Gibbs Sampling Algorithm
iters <- 10000
mu.save <- rep(0, iters)
mu.save[1] <- mu
sigma2.save <- rep(0, iters)
sigma2.save[1] <- sigma2

### Added for Metropolis RW
accept.mu <- 0
s.mu <- .25

#Gibbs Sampling Algorithm
for(t in 2:iters){
  
  # #What if the full conditional distribution was unknown??
  # 
  # # Full conditional of mu (update the value of the parameters)
  # lambda.p <- (tau2*sum(refract) + sigma2*lambda)/(tau2*n + sigma2)
  # tau2.p <- sigma2*tau2/(tau2*n + sigma2)
  
  # #sample a new value of mu
  # mu <- rnorm(1, lambda.p, sqrt(tau2.p))
  
  # #Then, use Metropolis RW to draw from the full conditional distribution...
  mu.star <- rnorm(1, mu, s.mu)
  mu.adj <- (mu-1)/4
  mu.star.adj <-(mu.star-1)/4
  
  if (0 < mu.star.adj && mu.star.adj < 1){
    beta.norm = (alpha - mu.adj*alpha)/mu.adj
    beta.star = (alpha - mu.star.adj*alpha)/mu.star.adj
    # cat("\nmu.adj", mu.adj)
    # cat("\nmu.star.adj", mu.star.adj)
    # cat("\nbeta.norm", beta.norm)
    # cat("\nbeta.star", beta.star)
    
    log.r <- sum(dbeta((pop2-1)/4, alpha, beta.star, log=T)) + 
             dunif(mu.star.adj, 0, 1, log=T) - #Proper prior calc
             sum(dbeta((pop2-1)/4, alpha, beta.norm, log=T)) - 
             dunif(mu.adj, 0, 1, log=T) #Proper prior calc
    
    logu <- log(runif(1))
    if(logu < log.r){
    	mu <- mu.star
    	accept.mu <- accept.mu + 1
    }
  }
  
  ### The below is golden
  #save the value of mu
  mu.save[t] <- mu
  
  # full conditional of sigma2 (update the value of the parameters)
  gamma.p <- gamma + n/2
  phi.p <- phi + sum((pop1 - mu)^2 )/2
  
  #sample new value of sigma2
  sigma2 <- rinvgamma(1, gamma.p, phi.p)
  #alternatively, instead of using the invgamma library, 
  #you could sample from the gamma distribution and invert the draw: 
  #sigma2 <- 1/rgamma(1, gamma.p, phi.p)
  
  #save the value of sigma2
  sigma2.save[t] <- sigma2
  
}

##Added to check acceptance rates
accept.mu/iters
```

```{r}
par(mfrow=c(1,2))
plot(mu.save, type='l')
plot(sigma2.save, type='l')
```

```{r}
#throw out the first few values
burn <- 100
mu.use <- mu.save[-(1:burn)]
sigma2.use <- sigma2.save[-(1:burn)]
plot(mu.use, type='l')
plot(sigma2.use, type='l')

#SUMMARIZE THE POSTERIOR DISTRIBUTION(S)

# joint posterior distribution of mu and sigma2
library(MASS)
par(mfrow=c(1,1))
joint.dens <- kde2d(mu.use, sigma2.use, n=100)
persp(joint.dens, xlab="mu", ylab="sigma2", phi=30, theta=45)

library(plot3D)
hist3D(joint.dens$x, joint.dens$y, joint.dens$z, xlab="mu", ylab="sigma2", phi=30, theta=45)

# posterior distribution of mu 
#plot 
plot(density(mu.use), xlab=expression(mu), ylab="density", main=expression(pi(mu~"|"~data)))
#add prior
curve(dnorm(x, lambda, sqrt(tau2)), lty=2, add=T)
#add legend
legend("topleft", c("Prior", "Posterior"), lty=c(2, 1))

#98% credible interval
quantile(mu.use, c(.01, .99))
#Given our data and prior knowledge, there is a 98% 
#chance that the true average refraction index on this 
# portion of the window is between 1.519 and 1.521.

#posterior mean of the average refraction index
mean(mu.use)

# posterior distribution of sigma2
par(mfrow=c(1,1))
plot(density(sigma2.use), xlab=expression(sigma^2), main=expression(pi(sigma^2~"|"~data)))
#add prior
curve(dinvgamma(x, gamma, phi), add=T, lty=2)
#add legend
legend("topright", c("Prior", "Posterior"), lty=c(2,1))

#98% credible interval for sigma2
quantile(sigma2.use, c(.01, .99))
#98% credible interval for sigma
quantile(sqrt(sigma2.use), c(.01, .99))
#There is a 98% chance, given our data and prior knowledge, 
#that the standard deviation of the refraction index on 
#this portion of the glass is between 0.0009 and 0.0024.

  
############################################
## Posterior predictive: pi(X*|data)
# We already have draws of (mu, sigma2) from the posterior distribution pi(mu, sigma2|data).  We can use those values to get draws of X*: 
x.star <- rnorm(length(mu.use), mu.use, sqrt(sigma2.use))  
plot(density(x.star), main="Posterior Predictive Distribution for Refraction Index of the Window", xlab="X*")

#95% credible interval for the refraction index of a randomly selected piece of glass from this portion of the window
quantile(x.star, c(.025, .975))
# Would it be reasonable to find a piece of glass from this portion of the window with a refraction index higher than 1.52004 (max in the data)?
mean(x.star>1.52004)
#given our data and prior knowledge, there is a 49% chance of observing a piece of glass with refraction index greater than 1.52004.  So yes, very reasonable.  
#(Note: more data would help us minimize that uncertainty for the range of refraction indices.)

#Question that might be interesting to explore: How can I make it so that this probability is very low? Play around with the prior distributions to see what happens.


```
